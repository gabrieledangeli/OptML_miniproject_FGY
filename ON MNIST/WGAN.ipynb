{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dange\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] Impossibile trovare la procedura specificata\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Importing needed libraries\n",
    "\n",
    "import torch, os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import grad\n",
    "from helpers import *\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from CGDs import ACGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.output_dim = output_dim \n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # The following layers are used to upsample the input\n",
    "        # noise and generate an output image.\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # This function initializes the weights and biases of the \n",
    "        # generator's layers:\n",
    "        initialize_weights(self) \n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        The input to the forward method represents the input noise \n",
    "        to the generator model. It returns the generated output image.\n",
    "        '''\n",
    "        \n",
    "        x = self.fc(input)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4)) # Reshapes x to match the expected size for the subsequent transposed convolutional layers. \n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # This function initializes the weights and biases of the \n",
    "        # discriminator's layers:\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        The input to the forward method represents the input image\n",
    "        It returns the output prediction, which represents the discriminator's assessment of the input image.\n",
    "        '''\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(object):\n",
    "    def __init__(self, epoch, batch_size, save_dir, result_dir, dataset, device, input_size, lrG, lrD, beta1, beta2):\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        self.result_dir = result_dir\n",
    "\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.input_size = input_size    # Default: 32\n",
    "\n",
    "        self.D_losses = []\n",
    "        self.G_losses = []\n",
    "        self.inception_scores = []\n",
    "        self.frechet_inception_distances = []\n",
    "\n",
    "        self.model_name = 'WGAN'        \n",
    "        self.z_dim = 62                 # Represents the dimensionality of the (Gaussian) random input vector used by the generator.\n",
    "        self.d_param_max = 0.01         # Clipping value\n",
    "        self.iteration_g_per_d = 5      # Specifies the number of iterations of the generator per discriminator iteration. \n",
    "                                        # This controls the relative training frequency of the generator and discriminator.\n",
    "\n",
    "        self.inception=InceptionScore(normalize=True)\n",
    "        self.f=FrechetInceptionDistance(feature=64,normalize=True)        \n",
    "        \n",
    "        if self.dataset == 'cifar10':\n",
    "                # Transformation to be applied on data in the DataLoader function\n",
    "                transform = transforms.Compose([\n",
    "                                        transforms.Resize((input_size, input_size)), \n",
    "                                        transforms.ToTensor(), \n",
    "                                        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.CIFAR10('data/cifar10', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        elif self.dataset == 'mnist':\n",
    "                # Transformation to be applied on data in the DataLoader function\n",
    "                transform = transforms.Compose([\n",
    "                                        transforms.Resize((input_size, input_size)), \n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "                                        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.MNIST('data/mnist', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        data = self.data_loader.__iter__().__next__()[0]\n",
    "\n",
    "        # Initialize generator, discriminator and optimizers\n",
    "        self.G = Generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size)\n",
    "        self.D = Discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size)\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "\n",
    "        if self.device == 'cuda:0':\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "        # Fixed noise: By setting up a fixed noise vector, the WGAN can generate consistent\n",
    "        # samples from the generator over the course of training. This allows for visualizing\n",
    "        # the progression of generated images or comparing them across different training iterations.\n",
    "        self.sample_z = torch.rand((self.batch_size, self.z_dim))\n",
    "        if self.device == 'cuda:0':\n",
    "            self.sample_z = self.sample_z.cuda()\n",
    "\n",
    "        print('---------------INITIALIZATION-----------------')\n",
    "        print('|           Model: WGAN with Adam            |')\n",
    "        print('----------------------------------------------')\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.y_real = torch.ones(self.batch_size, 1) # Initialize labels for real samples (i.e. 1)\n",
    "        self.y_fake = torch.zeros(self.batch_size, 1) # Initialize labels for fake samples (i.e. 0)\n",
    "\n",
    "        if self.device == 'cuda:0':\n",
    "            self.y_real = self.y_real.cuda()\n",
    "            self.y_fake = self.y_fake.cuda()\n",
    "\n",
    "        self.D.train() #D network in train mode\n",
    "\n",
    "        print('Start training!')\n",
    "        tot_iter = 0\n",
    "        for epoch in range(self.epoch):\n",
    "            iteration_losses_G = []     # Auxiliary arrays to compute the mean loss for each epoch\n",
    "            iteration_losses_D = []\n",
    "\n",
    "            self.G.train() #G network in train mode\n",
    "\n",
    "            for iter, (x_train, _) in enumerate(self.data_loader):\n",
    "                if iter == (self.data_loader.dataset.__len__() // self.batch_size): # Exit the loop when it has been used the whole dataset\n",
    "                    break\n",
    "\n",
    "                z_noise = torch.rand((self.batch_size, self.z_dim))\n",
    "\n",
    "                if self.device == 'cuda:0':\n",
    "                    x_train = x_train.cuda()\n",
    "                    z_noise = z_noise.cuda()\n",
    "\n",
    "                # PART 1: UPDATE THE DISCRIMINATOR\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x_train) # Computes the discriminator output for real data\n",
    "                D_real_loss = -torch.mean(D_real) # Calculates the loss for the discriminator on real data. \n",
    "                                                  # The objective is to maximize the average output of the discriminator for real data.\n",
    "\n",
    "                G_ = self.G(z_noise) # Generates fake data using the generator\n",
    "                D_fake = self.D(G_) # Computes the discriminator output for fake data.\n",
    "                D_fake_loss = torch.mean(D_fake) # Calculates the loss for the discriminator on fake data. \n",
    "                                                 # The objective is to minimize the average output of the discriminator for fake data.\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss # Calculates the total discriminator loss as the sum of the losses on real and fake data.\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                # Clipping D parameters to a specified range to enforce Lipschitz continuity.\n",
    "                for p in self.D.parameters():\n",
    "                    p.data.clamp_(-self.d_param_max, self.d_param_max)\n",
    "\n",
    "                if ((iter+1) % self.iteration_g_per_d) == 0:\n",
    "                    # PART 2: UPDATE THE GENERATOR\n",
    "                    self.G_optimizer.zero_grad()\n",
    "\n",
    "                    G_ = self.G(z_noise)\n",
    "                    D_fake = self.D(G_)\n",
    "                    G_loss = -torch.mean(D_fake) # Calculates the generator loss as the negative average output of the discriminator for fake data. \n",
    "                                                 # The objective is to maximize the average output of the discriminator for the generated fake data.\n",
    "\n",
    "                    iteration_losses_G.append(G_loss.item())\n",
    "\n",
    "                    G_loss.backward()\n",
    "\n",
    "                    self.G_optimizer.step()\n",
    "\n",
    "                    iteration_losses_D.append(D_loss.item())\n",
    "                \n",
    "                if tot_iter % 1000 == 0:        # Compute the Inception scores and the FID scores every 1000 iterations\n",
    "                     with torch.no_grad():\n",
    "                        score=self.IS()\n",
    "                        self.inception_scores.append(score[0].item())\n",
    "                        distance=self.FID()\n",
    "                        self.frechet_inception_distances.append(distance.item())\n",
    "                tot_iter += 1\n",
    "            \n",
    "            self.G_losses.append(np.mean(iteration_losses_G))\n",
    "            self.D_losses.append(np.mean(iteration_losses_D))\n",
    "\n",
    "            print(f'Epoch: {epoch+1}/{self.epoch}')\n",
    "            print(f'Discriminator loss: {self.D_losses[-1]}')\n",
    "            print(f'Generator loss: {self.G_losses[-1]}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.visualize_results((epoch+1))\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "        #self.save()\n",
    "        generate_animation(self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name,\n",
    "                                 self.epoch)\n",
    "        loss_plot(self.G_losses, self.D_losses, os.path.join(self.save_dir, self.dataset, self.model_name), self.model_name)\n",
    "        save_scores(self.inception_scores, self.save_dir, self.dataset, self.model_name, IS=True)\n",
    "        save_scores(self.frechet_inception_distances, self.save_dir, self.dataset, self.model_name, FID=True)\n",
    "        save_loss(self.G_losses, self.save_dir, self.dataset, self.model_name, Generator=True)\n",
    "        save_loss(self.D_losses, self.save_dir, self.dataset, self.model_name, Discriminator=True)\n",
    "    \n",
    "\n",
    "\n",
    "    def IS(self):\n",
    "        self.G.eval()\n",
    "        images = self.G(self.sample_z)\n",
    "        self.inception.update(images.cpu())\n",
    "        IS=self.inception.compute()   \n",
    "        return IS\n",
    "        \n",
    "\n",
    "\n",
    "    def FID(self):\n",
    "        self.G.eval()\n",
    "        imagesG = self.G(self.sample_z)\n",
    "        imagesR=self.data_loader.__iter__().__next__()[0]\n",
    "        self.f.update(imagesR.cpu(), real=True)\n",
    "        self.f.update(imagesG.cpu(), real=False)\n",
    "        FID=self.f.compute()\n",
    "        return FID\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_results(self, epoch):\n",
    "        self.G.eval()\n",
    "\n",
    "        if not os.path.exists(self.result_dir + '/' + self.dataset + '/' + self.model_name):\n",
    "            os.makedirs(self.result_dir + '/' + self.dataset + '/' + self.model_name)\n",
    "\n",
    "        samples = self.G(self.sample_z)\n",
    "        \n",
    "        if self.device == 'cuda:0':\n",
    "            samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "        else:\n",
    "            samples = samples.data.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "        samples = (samples + 1) / 2\n",
    "        save_images(samples, [8, 8],\n",
    "                          self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name + '_epoch%03d' % epoch + '.png')\n",
    "    \n",
    "\n",
    "\n",
    "    def print_networks(self):\n",
    "        print('---------- Networks architecture -------------')\n",
    "        print(self.G)\n",
    "        print(self.D)\n",
    "        print('-----------------------------------------------')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_GP(object):\n",
    "    def __init__(self, epoch, batch_size, save_dir, result_dir, dataset, device, input_size, lrG, lrD, beta1, beta2):\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        self.result_dir = result_dir\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.model_name = 'WGAN_GP'\n",
    "        self.z_dim = 62\n",
    "        self.lambda_ = 10\n",
    "        self.iteration_g_per_d = 5\n",
    "\n",
    "        \n",
    "        self.G_losses = []\n",
    "        self.D_losses = []\n",
    "        self.inception_scores = []\n",
    "        self.frechet_inception_distances = []\n",
    "\n",
    "        self.inception=InceptionScore(normalize=True)\n",
    "        self.f=FrechetInceptionDistance(feature=64,normalize=True)\n",
    "        \n",
    "        if self.dataset == 'cifar10':\n",
    "                # Transformation to be applied on data in the DataLoader function\n",
    "                transform = transforms.Compose([\n",
    "                                        transforms.Resize((input_size, input_size)), \n",
    "                                        transforms.ToTensor(), \n",
    "                                        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                                        ])           \n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.CIFAR10('data/cifar10', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        elif self.dataset == 'mnist':\n",
    "                # Transformation to be applied on data in the DataLoader function\n",
    "                transform = transforms.Compose([\n",
    "                                        transforms.Resize((input_size, input_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "                                        transforms.Normalize(mean=(0.5), std=(0.5))\n",
    "                                        ])\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.MNIST('data/mnist', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "                \n",
    "        data = self.data_loader.__iter__().__next__()[0]\n",
    "\n",
    "        # Initialization of optimizers and of generator and discriminator\n",
    "        self.G = Generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size)\n",
    "        self.D = Discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size)\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "\n",
    "        if self.device == 'cuda:0':\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "        # Fixed noise: By setting up a fixed noise vector, the WGAN can generate consistent\n",
    "        # samples from the generator over the course of training. This allows for visualizing\n",
    "        # the progression of generated images or comparing them across different training iterations.\n",
    "        self.sample_z = torch.rand((self.batch_size, self.z_dim))\n",
    "        if self.device == 'cuda:0':\n",
    "            self.sample_z = self.sample_z.cuda()\n",
    "        \n",
    "        print('---------------INITIALIZATION-----------------')\n",
    "        print('| Model: WGAN with Adam and Gradient Penalty |')\n",
    "        print('----------------------------------------------')\n",
    "\n",
    "\n",
    "    def train(self):    \n",
    "        self.y_real = torch.ones(self.batch_size, 1)\n",
    "        self.y_fake = torch.zeros(self.batch_size, 1)\n",
    "\n",
    "        if self.device == 'cuda:0':\n",
    "            self.y_real = self.y_real.cuda()\n",
    "            self.y_fake =  self.y_fake.cuda()\n",
    "\n",
    "        self.D.train() #Discriminator in train mode\n",
    "\n",
    "        print('Start training!')\n",
    "        tot_iter = 0\n",
    "        for epoch in range(self.epoch):\n",
    "            iteration_losses_G = []                                                  # Auxiliary arrays to compute the mean loss for each epoch\n",
    "            iteration_losses_D = []\n",
    "            \n",
    "            self.G.train()                                                           # Generator in train mode\n",
    "\n",
    "            for iter, (x_train, _) in enumerate(self.data_loader):\n",
    "                if iter == self.data_loader.dataset.__len__() // self.batch_size:    # Exit the loop when it has been used the whole dataset\n",
    "                    break\n",
    "\n",
    "                z_noise = torch.rand((self.batch_size, self.z_dim))\n",
    "                if self.device == 'cuda:0':\n",
    "                    x_train = x_train.cuda()\n",
    "                    z_noise = z_noise.cuda()\n",
    "\n",
    "                # PART 1: UPDATE THE DISCRIMINATOR\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x_train)\n",
    "                D_real_loss = -torch.mean(D_real)\n",
    "\n",
    "                G_ = self.G(z_noise)\n",
    "                D_fake = self.D(G_)\n",
    "                D_fake_loss = torch.mean(D_fake)\n",
    "\n",
    "                # Gradient penalty\n",
    "                alpha = torch.rand((self.batch_size, 1, 1, 1))\n",
    "                if self.device == 'cuda:0':\n",
    "                    alpha = alpha.cuda()\n",
    "\n",
    "                x_hat = alpha * x_train.data + (1 - alpha) * G_.data\n",
    "                x_hat.requires_grad = True\n",
    "\n",
    "                pred_hat = self.D(x_hat)\n",
    "                if self.device == 'cuda:0':\n",
    "                    gradients = grad(outputs=pred_hat, inputs=x_hat, grad_outputs=torch.ones(pred_hat.size()).cuda(),\n",
    "                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "                else:\n",
    "                    gradients = grad(outputs=pred_hat, inputs=x_hat, grad_outputs=torch.ones(pred_hat.size()),\n",
    "                                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "                gradient_penalty = self.lambda_ * ((gradients.view(gradients.size()[0], -1).norm(2, 1) - 1) ** 2).mean()\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss + gradient_penalty # Loss with gradient penalty\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                if ((iter+1) % self.iteration_g_per_d) == 0:\n",
    "                    # PART 2: UPDATE THE GENERATOR\n",
    "                    self.G_optimizer.zero_grad()\n",
    "\n",
    "                    G_ = self.G(z_noise)\n",
    "                    D_fake = self.D(G_)\n",
    "                    G_loss = -torch.mean(D_fake)\n",
    "\n",
    "                    iteration_losses_G.append(G_loss.item())\n",
    "\n",
    "                    G_loss.backward()\n",
    "                    self.G_optimizer.step()\n",
    "\n",
    "                    iteration_losses_D.append(D_loss.item())\n",
    "                \n",
    "                if tot_iter%1000 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        score=self.IS()\n",
    "                        self.inception_scores.append(score[0].item())\n",
    "                        distance=self.FID()\n",
    "                        self.frechet_inception_distances.append(distance.item())\n",
    "                tot_iter += 1\n",
    "\n",
    "            self.G_losses.append(np.mean(iteration_losses_G))\n",
    "            self.D_losses.append(np.mean(iteration_losses_D))\n",
    "\n",
    "            print(f'Epoch: {epoch+1}/{self.epoch}')\n",
    "            print(f'Average Discriminator loss: {self.D_losses[-1]}')\n",
    "            print(f'Average Generator loss: {self.G_losses[-1]}\\n')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.visualize_results((epoch+1))\n",
    "\n",
    "        print(\"Training finished!\")\n",
    "\n",
    "        #self.save()\n",
    "        generate_animation(self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name,\n",
    "                                 self.epoch)\n",
    "        loss_plot(self.G_losses, self.D_losses, os.path.join(self.save_dir, self.dataset, self.model_name), self.model_name)\n",
    "        save_scores(self.inception_scores, self.save_dir, self.dataset, self.model_name, IS=True)\n",
    "        save_scores(self.frechet_inception_distances, self.save_dir, self.dataset, self.model_name, FID=True)\n",
    "        save_loss(self.G_losses, self.save_dir, self.dataset, self.model_name, Generator=True)\n",
    "        save_loss(self.D_losses, self.save_dir, self.dataset, self.model_name, Discriminator=True)\n",
    "    \n",
    "\n",
    "\n",
    "    def IS(self):\n",
    "        self.G.eval()\n",
    "        images = self.G(self.sample_z)\n",
    "        self.inception.update(images.cpu())\n",
    "        IS=self.inception.compute()   \n",
    "        return IS\n",
    "    \n",
    "\n",
    "\n",
    "    def FID(self):\n",
    "        self.G.eval()\n",
    "        imagesG = self.G(self.sample_z)\n",
    "        imagesR=self.data_loader.__iter__().__next__()[0]\n",
    "        self.f.update(imagesR.cpu(), real=True)\n",
    "        self.f.update(imagesG.cpu(), real=False)\n",
    "        FID=self.f.compute()\n",
    "        return FID\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_results(self, epoch, fix=True):\n",
    "        '''\n",
    "        By calling this method during the training loop, you can visualize and save the \n",
    "        generated samples at regular intervals to monitor the progress \n",
    "        of the generator's output over the course of training.\n",
    "        '''\n",
    "        self.G.eval()                           # Set the generator in evaluation mode.\n",
    "\n",
    "        if not os.path.exists(self.result_dir + '/' + self.dataset + '/' + self.model_name):\n",
    "            os.makedirs(self.result_dir + '/' + self.dataset + '/' + self.model_name)\n",
    "\n",
    "        samples = self.G(self.sample_z)         # These samples represent the generator's output at the given epoch        \n",
    "        \n",
    "        # The dimensions are transposed to match the image format\n",
    "        if self.device == 'cuda:0':\n",
    "            samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "        else:\n",
    "            samples = samples.data.numpy().transpose(0, 2, 3, 1)\n",
    "        samples = (samples + 1) / 2             # Normalizes the samples by scaling their pixel \n",
    "                                                # values from the range [-1, 1] to the range [0, 1]\n",
    "        save_images(samples, [8, 8],\n",
    "                          self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name + '_epoch%03d' % epoch + '.png')\n",
    "\n",
    "\n",
    "\n",
    "    def print_networks(self):\n",
    "        print('---------- Networks architecture -------------')\n",
    "        print(self.G)\n",
    "        print(self.D)\n",
    "        print('-----------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_ACGD(object):\n",
    "    def __init__(self, epoch, batch_size, save_dir, result_dir, dataset, device, input_size, lrG, lrD):\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        self.result_dir = result_dir\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.model_name = 'WGAN_ACGD'\n",
    "        self.z_dim = 62\n",
    "        self.lambda_ = 10\n",
    "        self.d_param_max = 0.01\n",
    "\n",
    "        \n",
    "        self.losses = []\n",
    "        self.inception_scores = []\n",
    "        self.frechet_inception_distances = []\n",
    "        \n",
    "        self.inception=InceptionScore(normalize=True)\n",
    "        self.f=FrechetInceptionDistance(feature=64,normalize=True)               \n",
    "        \n",
    "        if self.dataset == 'cifar10':\n",
    "                # Transformation to be applied on data in the DataLoader function\n",
    "                transform = transforms.Compose([\n",
    "                                        transforms.Resize((input_size, input_size)), \n",
    "                                        transforms.ToTensor(), \n",
    "                                        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.CIFAR10('data/cifar10', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        elif self.dataset == 'mnist':\n",
    "                # Transformation to be applied on data in the DataLoader function\n",
    "                transform = transforms.Compose([\n",
    "                                        transforms.Resize((input_size, input_size)), \n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Lambda(lambda x: x.repeat(3, 1, 1)), \n",
    "                                        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.MNIST('data/mnist', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        data = self.data_loader.__iter__().__next__()[0]\n",
    "\n",
    "        # Initialization of optimizers and of generator and discriminator\n",
    "        self.G = Generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size)\n",
    "        self.D = Discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size)\n",
    "        self.optimizer = ACGD(max_params=self.G.parameters(), min_params=self.D.parameters(), lr_max=lrG, lr_min=lrD)\n",
    "        \n",
    "\n",
    "        if self.device == 'cuda:0':\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "        # Fixed noise: By setting up a fixed noise vector, the WGAN can generate consistent\n",
    "        # samples from the generator over the course of training. This allows for visualizing\n",
    "        # the progression of generated images or comparing them across different training iterations.\n",
    "        self.sample_z = torch.rand((self.batch_size, self.z_dim))\n",
    "        if self.device == 'cuda:0':\n",
    "            self.sample_z = self.sample_z.cuda()\n",
    "\n",
    "        print('----------------------INITIALIZATION------------------------')\n",
    "        print('| Model: WGAN using Competitive Gradient Descent optimizer |')\n",
    "        print('------------------------------------------------------------')\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        self.y_real = torch.ones(self.batch_size, 1)    # Initialize labels for real samples (i.e. 1)\n",
    "        self.y_fake = torch.zeros(self.batch_size, 1)   # Initialize labels for fake samples (i.e. 0)\n",
    "\n",
    "        if self.device == 'cuda:0':\n",
    "            self.y_real = self.y_real.cuda()\n",
    "            self.y_fake = self.y_fake.cuda()\n",
    "\n",
    "        self.D.train()                                  #D network in train mode\n",
    "\n",
    "        print('Start training!')\n",
    "        tot_iter = 0\n",
    "        for epoch in range(self.epoch):\n",
    "            self.G.train()                              # G network in train mode\n",
    "            iteration_losses = []                       # Auxilary array to compute the mean loss for each epoch\n",
    "            for iter, (x_train, _) in enumerate(self.data_loader):\n",
    "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    break\n",
    "\n",
    "                z_noise = torch.rand((self.batch_size, self.z_dim))\n",
    "\n",
    "                if self.device == 'cuda:0':\n",
    "                    x_train = x_train.cuda()\n",
    "                    z_noise = z_noise.cuda()\n",
    "\n",
    "                # UPDATE NETWORKS\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x_train)                # Computes the discriminator output for real data\n",
    "                D_real_loss = -torch.mean(D_real)       # Calculates the loss for the discriminator on real data. \n",
    "                                                        # The objective is to maximize the average output of the discriminator for real data.\n",
    "\n",
    "                G_ = self.G(z_noise)                    # Generates fake data using the generator\n",
    "                D_fake = self.D(G_)                     # Computes the discriminator output for fake data.\n",
    "                D_fake_loss = torch.mean(D_fake)        # Calculates the loss for the discriminator on fake data. \n",
    "                                                        # The objective is to minimize the average output of the discriminator for fake data.\n",
    "\n",
    "                loss = D_real_loss + D_fake_loss        # Calculates the total discriminator loss as the sum of the losses on real and fake data.\n",
    "                \n",
    "                iteration_losses.append(loss.detach().cpu())\n",
    "                self.optimizer.step(loss=loss)\n",
    "\n",
    "                # Clipping D parameters to a specified range to enforce Lipschitz continuity.\n",
    "                for p in self.D.parameters():\n",
    "                    p.data.clamp_(-self.d_param_max, self.d_param_max)\n",
    "                \n",
    "                if tot_iter%1000 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        score=self.IS()\n",
    "                        self.inception_scores.append(score[0].item())\n",
    "                        distance=self.FID()\n",
    "                        self.frechet_inception_distances.append(distance.item())\n",
    "                tot_iter += 1\n",
    "\n",
    "            self.losses.append(np.mean(iteration_losses))\n",
    "\n",
    "            print(f'Epoch: {epoch+1}/{self.epoch}')\n",
    "            print(f'Average loss: {self.losses[-1]}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.visualize_results((epoch+1))\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "        generate_animation(self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name,\n",
    "                                 self.epoch)\n",
    "        loss_plot_ACGD(self.losses, os.path.join(self.save_dir, self.dataset, self.model_name), self.model_name)\n",
    "        save_scores(self.inception_scores, self.save_dir, self.dataset, self.model_name, IS=True)\n",
    "        save_scores(self.frechet_inception_distances, self.save_dir, self.dataset, self.model_name, FID=True)\n",
    "        save_loss(self.losses, self.save_dir, self.dataset, self.model_name)\n",
    "\n",
    "\n",
    "    def visualize_results(self, epoch):\n",
    "        self.G.eval()\n",
    "\n",
    "        if not os.path.exists(self.result_dir + '/' + self.dataset + '/' + self.model_name):\n",
    "            os.makedirs(self.result_dir + '/' + self.dataset + '/' + self.model_name)\n",
    "\n",
    "        samples = self.G(self.sample_z)\n",
    "        if self.device == 'cuda:0':\n",
    "            samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "        else:\n",
    "            samples = samples.data.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "        samples = (samples + 1) / 2\n",
    "        save_images(samples, [8, 8],\n",
    "                          self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name + '_epoch%03d' % epoch + '.png')\n",
    "    \n",
    "\n",
    "\n",
    "    def IS(self):\n",
    "        self.G.eval()\n",
    "        images = self.G(self.sample_z)\n",
    "        self.inception.update(images.cpu())\n",
    "        IS=self.inception.compute()   \n",
    "        return IS\n",
    "\n",
    "\n",
    "\n",
    "    def FID(self):\n",
    "        self.G.eval()\n",
    "        imagesG = self.G(self.sample_z)\n",
    "        imagesR=self.data_loader.__iter__().__next__()[0]\n",
    "        self.f.update(imagesR.cpu(), real=True)\n",
    "        self.f.update(imagesG.cpu(), real=False)\n",
    "        FID=self.f.compute()\n",
    "        return FID\n",
    "    \n",
    "\n",
    "    def save_scores_loss(self):\n",
    "        save_scores\n",
    "\n",
    "\n",
    "    def print_networks(self):\n",
    "        print('---------- Networks architecture -------------')\n",
    "        print(self.G)\n",
    "        print(self.D)\n",
    "        print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dange\\anaconda3\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------INITIALIZATION-----------------\n",
      "|           Model: WGAN with Adam            |\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# Define dataset ['cifar10', 'mnist']\n",
    "dataset = 'mnist'\n",
    "\n",
    "# Define WGAN type [WGAN, WGAN_GP, WGAN_ACGD]\n",
    "gan_type = 'WGAN'\n",
    "\n",
    "# Define number of epoch\n",
    "epoch = 100\n",
    "\n",
    "# Define batch_size\n",
    "batch_size = 64\n",
    "\n",
    "# Define input_size\n",
    "input_size = 32\n",
    "\n",
    "# Define directory for saving results and models\n",
    "save_dir = 'models'\n",
    "result_dir = 'results'\n",
    "\n",
    "\n",
    "# Defining learning rates\n",
    "lrG = 0.0001        # Generator\n",
    "lrD = 0.0001        # Discriminator\n",
    "\n",
    "# Adam beta parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "\n",
    "# 'cuda' if GPU is available, 'cpu' else\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if gan_type == 'WGAN':\n",
    "    wgan = WGAN(epoch, batch_size, save_dir, result_dir, dataset, device, input_size, lrG, lrD, beta1, beta2)\n",
    "elif gan_type == 'WGAN_GP':\n",
    "    wgan = WGAN_GP(epoch, batch_size, save_dir, result_dir, dataset, device, input_size, lrG, lrD, beta1, beta2)\n",
    "elif gan_type == 'WGAN_ACGD':\n",
    "    wgan = WGAN_ACGD(epoch, batch_size, save_dir, result_dir, dataset, device, input_size, lrG, lrD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
