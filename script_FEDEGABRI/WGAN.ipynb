{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dange\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] Impossibile trovare la procedura specificata\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import grad\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from skimage.transform import resize\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n",
    "        x = self.deconv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(object):\n",
    "    def __init__(self, epoch, batch_size, save_dir, result_dir, dataset, log_dir, gpu_mode, gan_type, input_size, lrG, lrD, beta1, beta2):\n",
    "        # parameters\n",
    "        self.epoch = epoch\n",
    "        self.sample_num = 100\n",
    "        self.batch_size = batch_size\n",
    "        self.save_dir = save_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.dataset = dataset\n",
    "        self.log_dir = log_dir\n",
    "        self.gpu_mode = gpu_mode\n",
    "        self.model_name = gan_type\n",
    "        self.input_size = input_size\n",
    "        self.z_dim = 62\n",
    "        self.c = 0.01                   # clipping value\n",
    "        self.n_critic = 5               # the number of iterations of the critic per generator iteration\n",
    "\n",
    "\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "                                        transforms.Resize((input_size, input_size)), \n",
    "                                        transforms.ToTensor(), \n",
    "                                        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "        if self.dataset == 'mnist':\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.MNIST('data/mnist', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        elif self.dataset == 'cifar10':\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.CIFAR10('data/cifar10', train=True, download=False, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        data = self.data_loader.__iter__().__next__()[0]\n",
    "\n",
    "        # Networks init\n",
    "        self.G = Generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size)\n",
    "        self.D = Discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size)\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=lrG, betas= (beta1, beta2))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=lrD, betas= (beta1, beta2))\n",
    "\n",
    "        if self.gpu_mode:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "        print('---------- Networks architecture -------------')\n",
    "        print(self.G)\n",
    "        print(self.D)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        # fixed noise\n",
    "        self.sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "        if self.gpu_mode:\n",
    "            self.sample_z_ = self.sample_z_.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_loss'] = []\n",
    "        self.train_hist['G_loss'] = []\n",
    "        self.train_hist['per_epoch_time'] = []\n",
    "        self.train_hist['total_time'] = []\n",
    "\n",
    "        self.y_real_ = torch.ones(self.batch_size, 1)\n",
    "        self.y_fake_ = torch.zeros(self.batch_size, 1)\n",
    "\n",
    "        if self.gpu_mode:\n",
    "            self.y_real_ = self.y_real_.cuda()\n",
    "            self.y_fake_ = self.y_fake_.cuda()\n",
    "\n",
    "        self.D.train()\n",
    "\n",
    "        print('Start training!')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epoch):\n",
    "            self.G.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for iter, (x_train, _) in enumerate(self.data_loader):\n",
    "\n",
    "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    break\n",
    "\n",
    "                z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "                if self.gpu_mode:\n",
    "                    x_train, z_ = x_train.cuda(), z_.cuda()\n",
    "\n",
    "                # update D network\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x_train)\n",
    "                D_real_loss = -torch.mean(D_real)\n",
    "\n",
    "                G_ = self.G(z_)\n",
    "                D_fake = self.D(G_)\n",
    "                D_fake_loss = torch.mean(D_fake)\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                # Clipping D\n",
    "                for p in self.D.parameters():\n",
    "                    p.data.clamp_(-self.c, self.c)\n",
    "\n",
    "                if ((iter+1) % self.n_critic) == 0:\n",
    "                    # Update G network\n",
    "                    self.G_optimizer.zero_grad()\n",
    "\n",
    "                    G_ = self.G(z_)\n",
    "                    D_fake = self.D(G_)\n",
    "                    G_loss = -torch.mean(D_fake)\n",
    "                    self.train_hist['G_loss'].append(G_loss.item())\n",
    "\n",
    "                    G_loss.backward()\n",
    "                    self.G_optimizer.step()\n",
    "\n",
    "                    self.train_hist['D_loss'].append(D_loss.item())\n",
    "\n",
    "                if ((iter + 1) % 100) == 0:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
    "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
    "\n",
    "            self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)\n",
    "            with torch.no_grad():\n",
    "                self.visualize_results((epoch+1))\n",
    "\n",
    "        self.train_hist['total_time'].append(time.time() - start_time)\n",
    "        print(\"Avg one epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.train_hist['per_epoch_time']),\n",
    "              self.epoch, self.train_hist['total_time'][0]))\n",
    "        print(\"Training finish!... save training results\")\n",
    "\n",
    "        #self.save()\n",
    "        generate_animation(self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name,\n",
    "                                 self.epoch)\n",
    "        loss_plot(self.train_hist, os.path.join(self.save_dir, self.dataset, self.model_name), self.model_name)\n",
    "    \n",
    "    def calculateInceptionScore(self,fix=True):\n",
    "        self.G.eval()\n",
    "        if fix:\n",
    "            \"\"\" fixed noise \"\"\"\n",
    "            samples = self.G(self.sample_z_)\n",
    "        else:\n",
    "            \"\"\" random noise \"\"\"\n",
    "            sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "            if self.gpu_mode:\n",
    "                sample_z_ = sample_z_.cuda()\n",
    "\n",
    "            samples = self.G(sample_z_)\n",
    "        \n",
    "        eps=1E-16\n",
    "        scores=[]\n",
    "        samples=samples.cpu().data.numpy()\n",
    "        samples=scale_images(samples, (299,299,3))\n",
    "        samples=preprocess_input(samples)\n",
    "        model=InceptionV3()\n",
    "        p_yx=model.predict(samples)\n",
    "        p_y = np.expand_dims(p_yx.mean(axis=0), 0)\n",
    "        kl_d = p_yx * (np.log(p_yx + eps) - np.log(p_y + eps))\n",
    "        sum_kl_d = kl_d.sum(axis=1)\n",
    "        avg_kl_d = np.mean(sum_kl_d)\n",
    "        is_score = np.exp(avg_kl_d)\n",
    "        scores.append(is_score)\n",
    "        return scores\n",
    "        \n",
    "\n",
    "    def visualize_results(self, epoch, fix=True):\n",
    "        self.G.eval()\n",
    "\n",
    "        if not os.path.exists(self.result_dir + '/' + self.dataset + '/' + self.model_name):\n",
    "            os.makedirs(self.result_dir + '/' + self.dataset + '/' + self.model_name)\n",
    "\n",
    "        tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "\n",
    "        if fix:\n",
    "            \"\"\" fixed noise \"\"\"\n",
    "            samples = self.G(self.sample_z_)\n",
    "        else:\n",
    "            \"\"\" random noise \"\"\"\n",
    "            sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "            if self.gpu_mode:\n",
    "                sample_z_ = sample_z_.cuda()\n",
    "            samples = self.G(sample_z_)\n",
    "\n",
    "        if self.gpu_mode:\n",
    "            samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "        else:\n",
    "            samples = samples.data.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "        samples = (samples + 1) / 2\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                          self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name + '_epoch%03d' % epoch + '.png')\n",
    "    '''\n",
    "    def save(self):\n",
    "        save_dir = os.path.join(self.save_dir, self.dataset, self.model_name)\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        torch.save(self.G.state_dict(), os.path.join(save_dir, self.model_name + '_G.pkl'))\n",
    "        torch.save(self.D.state_dict(), os.path.join(save_dir, self.model_name + '_D.pkl'))\n",
    "\n",
    "        with open(os.path.join(save_dir, self.model_name + '_history.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.train_hist, f)\n",
    "\n",
    "    def load(self):\n",
    "        save_dir = os.path.join(self.save_dir, self.dataset, self.model_name)\n",
    "\n",
    "        self.G.load_state_dict(torch.load(os.path.join(save_dir, self.model_name + '_G.pkl')))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(save_dir, self.model_name + '_D.pkl')))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_GP(object):\n",
    "    def __init__(self, epoch, batch_size, save_dir, result_dir, dataset, log_dir, gpu_mode, gan_type, input_size, lrG, lrD, beta1, beta2):\n",
    "        # parameters\n",
    "        self.epoch = epoch\n",
    "        self.sample_num = 100\n",
    "        self.batch_size = batch_size\n",
    "        self.save_dir = save_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.dataset = dataset\n",
    "        self.log_dir = log_dir\n",
    "        self.gpu_mode = gpu_mode\n",
    "        self.model_name = gan_type\n",
    "        self.input_size = input_size\n",
    "        self.z_dim = 62\n",
    "        self.lambda_ = 10\n",
    "        self.n_critic = 5               # the number of iterations of the critic per Generator iteration\n",
    "\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "                                transforms.Resize((self.input_size, self.input_size)), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "        if self.dataset == 'mnist':\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.MNIST('data/mnist', train=True, download=True, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        elif self.dataset == 'cifar10':\n",
    "                self.data_loader = DataLoader(\n",
    "                    datasets.CIFAR10('data/cifar10', train=True, download=False, transform=transform),\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "        data = self.data_loader.__iter__().__next__()[0]\n",
    "\n",
    "        # networks init\n",
    "        self.G = Generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size)\n",
    "        self.D = Discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size)\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "\n",
    "        if self.gpu_mode:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "        print('---------- Networks architecture -------------')\n",
    "        print(self.G)\n",
    "        print(self.D)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        # fixed noise\n",
    "        self.sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "        if self.gpu_mode:\n",
    "            self.sample_z_ = self.sample_z_.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_loss'] = []\n",
    "        self.train_hist['G_loss'] = []\n",
    "        self.train_hist['per_epoch_time'] = []\n",
    "        self.train_hist['total_time'] = []\n",
    "\n",
    "        self.y_real_ = torch.ones(self.batch_size, 1)\n",
    "        self.y_fake_ = torch.zeros(self.batch_size, 1)\n",
    "        if self.gpu_mode:\n",
    "            self.y_real_ = self.y_real_.cuda()\n",
    "            self.y_fake_ =  self.y_fake_.cuda()\n",
    "\n",
    "        self.D.train()\n",
    "        print('Start training')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epoch):\n",
    "            self.G.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for iter, (x_train, _) in enumerate(self.data_loader):\n",
    "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    break\n",
    "\n",
    "                z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "                if self.gpu_mode:\n",
    "                    x_train, z_ = x_train.cuda(), z_.cuda()\n",
    "\n",
    "                # update D network\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x_train)\n",
    "                D_real_loss = -torch.mean(D_real)\n",
    "\n",
    "                G_ = self.G(z_)\n",
    "                D_fake = self.D(G_)\n",
    "                D_fake_loss = torch.mean(D_fake)\n",
    "\n",
    "                # gradient penalty\n",
    "                alpha = torch.rand((self.batch_size, 1, 1, 1))\n",
    "                if self.gpu_mode:\n",
    "                    alpha = alpha.cuda()\n",
    "\n",
    "                x_hat = alpha * x_train.data + (1 - alpha) * G_.data\n",
    "                x_hat.requires_grad = True\n",
    "\n",
    "                pred_hat = self.D(x_hat)\n",
    "                if self.gpu_mode:\n",
    "                    gradients = grad(outputs=pred_hat, inputs=x_hat, grad_outputs=torch.ones(pred_hat.size()).cuda(),\n",
    "                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "                else:\n",
    "                    gradients = grad(outputs=pred_hat, inputs=x_hat, grad_outputs=torch.ones(pred_hat.size()),\n",
    "                                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "                gradient_penalty = self.lambda_ * ((gradients.view(gradients.size()[0], -1).norm(2, 1) - 1) ** 2).mean()\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss + gradient_penalty\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                if ((iter+1) % self.n_critic) == 0:\n",
    "                    # update G network\n",
    "                    self.G_optimizer.zero_grad()\n",
    "\n",
    "                    G_ = self.G(z_)\n",
    "                    D_fake = self.D(G_)\n",
    "                    G_loss = -torch.mean(D_fake)\n",
    "                    self.train_hist['G_loss'].append(G_loss.item())\n",
    "\n",
    "                    G_loss.backward()\n",
    "                    self.G_optimizer.step()\n",
    "\n",
    "                    self.train_hist['D_loss'].append(D_loss.item())\n",
    "\n",
    "                if ((iter + 1) % 100) == 0:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
    "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
    "\n",
    "            self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)\n",
    "            with torch.no_grad():\n",
    "                self.visualize_results((epoch+1))\n",
    "\n",
    "        self.train_hist['total_time'].append(time.time() - start_time)\n",
    "        print(\"Avg one epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.train_hist['per_epoch_time']),\n",
    "              self.epoch, self.train_hist['total_time'][0]))\n",
    "        print(\"Training finish!... save training results\")\n",
    "\n",
    "        #self.save()\n",
    "        generate_animation(self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name,\n",
    "                                 self.epoch)\n",
    "        loss_plot(self.train_hist, os.path.join(self.save_dir, self.dataset, self.model_name), self.model_name)\n",
    "    \n",
    "    def calculateInceptionScore(self,fix=True):\n",
    "        self.G.eval()\n",
    "        if fix:\n",
    "            \"\"\" fixed noise \"\"\"\n",
    "            samples = self.G(self.sample_z_)\n",
    "        else:\n",
    "            \"\"\" random noise \"\"\"\n",
    "            sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "            if self.gpu_mode:\n",
    "                sample_z_ = sample_z_.cuda()\n",
    "\n",
    "            samples = self.G(sample_z_)\n",
    "        \n",
    "        eps=1E-16\n",
    "        scores=[]\n",
    "        samples=samples.cpu().data.numpy()\n",
    "        samples=scale_images(samples, (299,299,3))\n",
    "        samples=preprocess_input(samples)\n",
    "        model=InceptionV3()\n",
    "        p_yx=model.predict(samples)\n",
    "        p_y = expand_dims(p_yx.mean(axis=0), 0)\n",
    "        kl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n",
    "        sum_kl_d = kl_d.sum(axis=1)\n",
    "        avg_kl_d = mean(sum_kl_d)\n",
    "        is_score = exp(avg_kl_d)\n",
    "        scores.append(is_score)\n",
    "        return scores\n",
    "        \n",
    "    def visualize_results(self, epoch, fix=True):\n",
    "        self.G.eval()\n",
    "\n",
    "        if not os.path.exists(self.result_dir + '/' + self.dataset + '/' + self.model_name):\n",
    "            os.makedirs(self.result_dir + '/' + self.dataset + '/' + self.model_name)\n",
    "\n",
    "        tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "\n",
    "        if fix:\n",
    "            \"\"\" fixed noise \"\"\"\n",
    "            samples = self.G(self.sample_z_)\n",
    "        else:\n",
    "            \"\"\" random noise \"\"\"\n",
    "            sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "            if self.gpu_mode:\n",
    "                sample_z_ = sample_z_.cuda()\n",
    "\n",
    "            samples = self.G(sample_z_)\n",
    "\n",
    "        if self.gpu_mode:\n",
    "            samples = samples.cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "        else:\n",
    "            samples = samples.data.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "        samples = (samples + 1) / 2\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                          self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name + '_epoch%03d' % epoch + '.png')\n",
    "    '''\n",
    "    def save(self):\n",
    "        save_dir = os.path.join(self.save_dir, self.dataset, self.model_name)\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        torch.save(self.G.state_dict(), os.path.join(save_dir, self.model_name + '_G.pkl'))\n",
    "        torch.save(self.D.state_dict(), os.path.join(save_dir, self.model_name + '_D.pkl'))\n",
    "\n",
    "        with open(os.path.join(save_dir, self.model_name + '_history.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.train_hist, f)\n",
    "\n",
    "    def load(self):\n",
    "        save_dir = os.path.join(self.save_dir, self.dataset, self.model_name)\n",
    "\n",
    "        self.G.load_state_dict(torch.load(os.path.join(save_dir, self.model_name + '_G.pkl')))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(save_dir, self.model_name + '_D.pkl')))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks architecture -------------\n",
      "Generator(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=62, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=8192, bias=True)\n",
      "    (4): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (deconv): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "-----------------------------------------------\n",
      "Start training\n",
      "Epoch: [ 1] [ 100/ 781] D_loss: 0.04035524, G_loss: 0.66510689\n",
      "Epoch: [ 1] [ 200/ 781] D_loss: -0.49660957, G_loss: 0.99399287\n",
      "Epoch: [ 1] [ 300/ 781] D_loss: -0.64450598, G_loss: 2.93423557\n",
      "Epoch: [ 1] [ 400/ 781] D_loss: -0.98690343, G_loss: 3.74724197\n",
      "Epoch: [ 1] [ 500/ 781] D_loss: -1.94948745, G_loss: 3.71105337\n",
      "Epoch: [ 1] [ 600/ 781] D_loss: -2.35151958, G_loss: 4.47642040\n",
      "Epoch: [ 1] [ 700/ 781] D_loss: -1.56066537, G_loss: 4.67948818\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39melif\u001b[39;00m gan_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mWGAN_GP\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     19\u001b[0m     wgan \u001b[39m=\u001b[39m WGAN_GP(epoch, batch_size, save_dir, result_dir, dataset, log_dir, gpu_mode, gan_type, input_size, lrG, lrD, beta1, beta2)\n\u001b[1;32m---> 22\u001b[0m wgan\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[5], line 134\u001b[0m, in \u001b[0;36mWGAN_GP.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_hist[\u001b[39m'\u001b[39m\u001b[39mper_epoch_time\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m epoch_start_time)\n\u001b[0;32m    133\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisualize_results((epoch\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_hist[\u001b[39m'\u001b[39m\u001b[39mtotal_time\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\n\u001b[0;32m    137\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAvg one epoch time: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m, total \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m epochs time: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (np\u001b[39m.\u001b[39mmean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_hist[\u001b[39m'\u001b[39m\u001b[39mper_epoch_time\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[0;32m    138\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_hist[\u001b[39m'\u001b[39m\u001b[39mtotal_time\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[5], line 200\u001b[0m, in \u001b[0;36mWGAN_GP.visualize_results\u001b[1;34m(self, epoch, fix)\u001b[0m\n\u001b[0;32m    197\u001b[0m     samples \u001b[39m=\u001b[39m samples\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    199\u001b[0m samples \u001b[39m=\u001b[39m (samples \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m--> 200\u001b[0m utils\u001b[39m.\u001b[39msave_images(samples[:image_frame_dim \u001b[39m*\u001b[39m image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n\u001b[0;32m    201\u001b[0m                   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_epoch\u001b[39m\u001b[39m%03d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m epoch \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar10'\n",
    "gan_type = 'WGAN_GP'\n",
    "epoch = 1\n",
    "batch_size = 64\n",
    "input_size = 32\n",
    "save_dir = 'models'\n",
    "result_dir = 'results'\n",
    "log_dir = 'logs'\n",
    "lrG = 0.0002\n",
    "lrD = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "gpu_mode =True\n",
    "\n",
    "\n",
    "if gan_type == 'WGAN':\n",
    "    wgan = WGAN(epoch, batch_size, save_dir, result_dir, dataset, log_dir, gpu_mode, gan_type, input_size, lrG, lrD, beta1, beta2)\n",
    "elif gan_type == 'WGAN_GP':\n",
    "    wgan = WGAN_GP(epoch, batch_size, save_dir, result_dir, dataset, log_dir, gpu_mode, gan_type, input_size, lrG, lrD, beta1, beta2)\n",
    "\n",
    "\n",
    "wgan.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
